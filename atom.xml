<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-07T13:20:41.000Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>ohh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Kafka环境配置》</title>
    <link href="http://example.com/2022/06/07/kafak1/"/>
    <id>http://example.com/2022/06/07/kafak1/</id>
    <published>2022-06-07T13:20:41.000Z</published>
    <updated>2022-06-07T13:20:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《Kafka环境配置》"><a href="#《Kafka环境配置》" class="headerlink" title="《Kafka环境配置》"></a>《Kafka环境配置》</h1><hr><p>title: 《Kafka环境配置》<br>date: 2022-6-7 21:20:41<br>description: Spark (HA) Spark (Yarn)</p><details><summary>阅读全文</summary><p>**<summary><br>kafka 安装配置<br>Kafka是一种高吞吐量的分布式发布订阅消息系统，其在大数据开发应用上的目的是通过 Hadoo的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。大数据开发需掌握Kafka架构原理及各组件的作用和使用方法及相关功能的实现。</p><p>上传文件包 到&#x2F;export&#x2F;server&#x2F;<br>解压文件</p><blockquote><blockquote><blockquote></blockquote><pre><code>tar -zxvf kafka_2.11-2.0.0.tgz</code></pre></blockquote></blockquote><p>创建软连接</p><blockquote><blockquote><blockquote></blockquote><pre><code>ln -s kafka_2.11-2.0.0/ kafka</code></pre></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config 修改 配置文件 server.properties</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config vim server.properties</p></blockquote></blockquote><p>(1)21 行内容 broker.id&#x3D;0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复<br>（注：此处不用修改）</p><blockquote><blockquote><blockquote></blockquote><p>   21 broker.id&#x3D;0</p></blockquote></blockquote><p>(2)31 行内容 #listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092 取消注释，内容改为：<br>listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;master:9092<br>PLAINTEXT为通信使用明文（加密ssl）</p><blockquote><blockquote><blockquote></blockquote><p>   31 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;master:9092</p></blockquote></blockquote><p>(3)59 行内容 log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs 为默认日志文件存储的位置，改为<br>log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs</p><blockquote><blockquote><blockquote></blockquote><p>   59 log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs</p></blockquote></blockquote><p>(4)63 行内容为 num.partitions&#x3D;1 是默认分区数</p><blockquote><blockquote><blockquote></blockquote><pre><code>63 num.partitions=1</code></pre></blockquote></blockquote><p>(5)76 行部分</p><blockquote><blockquote><blockquote><p>  ############################ <strong>Log Flush Policy</strong> ############################### </p></blockquote></blockquote></blockquote><p>数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上）interval.messages interval.ms</p><p>(6)93 行部分</p><blockquote><blockquote><blockquote></blockquote><pre><code>########################### **Log Retention Policy**############################ </code></pre></blockquote></blockquote><pre><code>数据保留策略 168/24=7，1073741824/1024=1GB，300000ms = 300s = 5min超过了删掉 （最后修改时间还是创建时间--&gt;日志段中最晚的一条消息，维护这个最大的时间戳--&gt;用户无法 干预</code></pre><p>(7)121 行内容 zookeeper.connect&#x3D;localhost:2181 修改为zookeeper.connect&#x3D;master:2181,slave1:2181,slave2:2181</p><blockquote><blockquote><blockquote></blockquote><p>   121 zookeeper.connect&#x3D;master:2181,slave1:2181,slave2:2181</p></blockquote></blockquote><p>(8)126 行内容 group.initial.rebalance.delay.ms&#x3D;0 修改为group.initial.rebalance.delay.ms&#x3D;3000</p><blockquote><blockquote><blockquote></blockquote><p>   133 group.initial.rebalance.delay.ms&#x3D;3000</p></blockquote></blockquote><p>(9)给 slaves1和 slavs2 scp 分发 kafka<br>cd &#x2F;export&#x2F;server&#x2F; </p><blockquote><blockquote><blockquote></blockquote><p>   scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; slave1:$PWD<br>   scp -r &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; slave2:$PWD</p></blockquote></blockquote><p>(10)创建软连接</p><blockquote><blockquote><blockquote><p>   ln -s &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0&#x2F; kafka</p></blockquote></blockquote></blockquote><p>(11)配置 kafka 环境变量（注：可以一台一台配，也可以在 master 完成后发给 slave1 和slave2）</p><blockquote><blockquote><blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile # kafka 环境变量 export KAFKA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin</p></blockquote></blockquote><p>(12)重新加载环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>(13)分别在 slave1 和slave2 上修改配置文件 路径：&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config<br>将文件 server.properties 的第 21 行的 broker.id&#x3D;0 修改为 broker.id&#x3D;1 同理 slave2 同样操<br>作</p><blockquote><blockquote><blockquote></blockquote><p>   21 broker.id&#x3D;1</p></blockquote></blockquote><p>(14)将文件 server.properties 的第 31 行的 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;master:9092 修改为<br>listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;slave1:9092 同理slave2 同样操作</p><blockquote><blockquote><blockquote></blockquote><p>   31 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;slave1:9092</p></blockquote></blockquote><p>(15)启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可)</p><blockquote><blockquote><blockquote></blockquote><p>   kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties</p></blockquote></blockquote><p>   hadoop，zookeeper，kafka启动<br>   结果显示： </p><blockquote><blockquote><blockquote></blockquote><p>   (base) [root@master ~]# jps<br>   11793 NodeManager<br>   91699 Kafka<br>   85618 QuorumPeerMain<br>   10697 NameNode<br>   10924 DataNode<br>   11596ResourceManager<br>   109852 Jps </p></blockquote></blockquote><blockquote><blockquote><blockquote></blockquote><p>   [root@slave1 ~]# jps<br>   9301 DataNode<br>   9493 SecondaryNameNode<br>   95959 Kafka<br>   102971 Jps<br>   9855 NodeManager<br>   89534 QuorumPeerMain </p></blockquote></blockquote><blockquote><blockquote><blockquote></blockquote><p>   [root@slave2 ~]# jps<br>   88660 QuorumPeerMain<br>   95204 Kafka<br>   9110 NodeManager<br>   8616 DataNode<br>   102104 Jps</p></blockquote></blockquote><p>   关闭 kafka<br>   kafka-server-stop.sh stop</p><p>定制脚本一键启动</p><blockquote><blockquote><blockquote></blockquote><p>   vim kafka-all.sh<br>   放入 &#x2F;bin 路径下</p></blockquote></blockquote><blockquote><blockquote><blockquote></blockquote><p>   #!&#x2F;bin&#x2F;bash<br>   if [ $# -eq 0 ] ;<br>   then<br>       echo “please input param:start stop”<br>   else<br>   if [ $1 &#x3D; start ] ;then<br>       echo “${1}ing master”<br>       ssh master “source &#x2F;etc&#x2F;profile;kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties”<br>       for i in {1..2}<br>       do<br>          echo “${1}ing slave${i}”<br>          ssh slave${i} “source &#x2F;etc&#x2F;profile;kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties”<br>      done<br>   fi<br>   if [ $1 &#x3D; stop ];then<br>       echo “${1}ping master “<br>       ssh master “source &#x2F;etc&#x2F;profile;kafka-server-stop.sh”<br>      for i in {1..2}<br>      do<br>         echo “${1}ping slave${i}”<br>         ssh slave${i} “source &#x2F;etc&#x2F;profile;kafka-server-stop.sh”<br>      done<br>   fi<br>   fi<br>**</p></blockquote></blockquote></details><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《Kafka环境配置》&quot;&gt;&lt;a href=&quot;#《Kafka环境配置》&quot; class=&quot;headerlink&quot; title=&quot;《Kafka环境配置》&quot;&gt;&lt;/a&gt;《Kafka环境配置》&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;title: 《Kafka环境配置》&lt;br&gt;date: 2</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>《Kafka命令行操作》</title>
    <link href="http://example.com/2022/06/07/kafka2/"/>
    <id>http://example.com/2022/06/07/kafka2/</id>
    <published>2022-06-07T13:20:41.000Z</published>
    <updated>2022-06-07T13:20:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《Kafka命令行操作》"><a href="#《Kafka命令行操作》" class="headerlink" title="《Kafka命令行操作》"></a>《Kafka命令行操作》</h1><hr><p>title: 《Kafka命令行操作》<br>date: 2022-6-7 21:20:41<br>description: Spark (HA) Spark (Yarn)</p><details><summary>阅读全文</summary><p>**<summary></p><p>1、cd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;bin目录下的命令行工具</p><blockquote><blockquote><blockquote></blockquote><pre><code>kafka-configs.sh用于配置管理kafka-console-consumer.sh用于消费消息kafka-console-producer.sh用于生产消息kafka-consumer-perf-test.sh用于测试消费性能kafka-topics.sh用于管理主题kafka-dump-log.sh用于查看日志内容kafka-server-stop.sh用于关闭kafka服务kafka-preferred-replica-election.sh用于优先副本的选举kafka-server-start.sh用于启动kafka服务kafka-producer-perf-test.sh用于测试生产性能kafka-reassign-partitions.sh用于分区重分配</code></pre></blockquote></blockquote><p>2、查看当前可用topic（应该什么也没有）</p><p><img src="/../images/1.png" alt="1"></p><p>3、创建topic，检查是否创建成功<br><img src="/../images/2.png" alt="2"></p><p>4、手动指定副本的存储位置，进入cd &#x2F;export&#x2F;data&#x2F;kafka-logs路径下查看分区的分布<br><img src="/../images/3.png" alt="3"><br><img src="/../images/4.png" alt="4"><br><img src="/../images/5.png" alt="5"><br><img src="/../images/6.png" alt="6"></p><p>5、进入到zookeeper client，查看目录、controller、controller_epoch<br><img src="/../images/7.png" alt="7"></p><p>6、删除topic，检查是否删除成功<br><img src="/../images/8.png" alt="8"><br><img src="/../images/9.png" alt="9"><br><img src="/../images/10.png" alt="10"><br><img src="/../images/11.png" alt="11"></p><p>7、查看topic详情<br><img src="/../images/12.png" alt="12"></p><p>8、增加分区数，查看topic详情<br><img src="/../images/13.png" alt="13"></p><p>9、修改参数<br><img src="/../images/14.png" alt="14"></p><p>10、生产者写入数据、消费者拉取数据<br><img src="/../images/15.png" alt="15"><br><img src="/../images/16.png" alt="16"><br>**</p></details><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《Kafka命令行操作》&quot;&gt;&lt;a href=&quot;#《Kafka命令行操作》&quot; class=&quot;headerlink&quot; title=&quot;《Kafka命令行操作》&quot;&gt;&lt;/a&gt;《Kafka命令行操作》&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;title: 《Kafka命令行操作》&lt;br&gt;da</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>《kafka API使用方法》</title>
    <link href="http://example.com/2022/06/07/kafka3/"/>
    <id>http://example.com/2022/06/07/kafka3/</id>
    <published>2022-06-07T13:20:41.000Z</published>
    <updated>2022-06-07T13:20:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《kafka-API使用方法》"><a href="#《kafka-API使用方法》" class="headerlink" title="《kafka API使用方法》"></a>《kafka API使用方法》</h1><hr><p>title: 《kafka API使用方法》<br>date: 2022-6-7 21:20:41<br>description: Spark (HA) Spark (Yarn)</p><details><summary>阅读全文</summary>1、一个正常的生产逻辑需要具备以下几个步骤   >>>     （1）、配置生产者客户端参数及创建相应的生产者实例；    （2）、构建待发送的消息；    （3）、发送消息；    （4）、关闭生产者实例。<p>2、生产者API采用默认分区方式将消息散列的发送到各个分区中。</p><p>3、什么时候会发生重复写入：producer的重试机制中，检测到一条数据的发送失败，而实际上已经发送成功，只是因为服务端响应超时。</p><p>4、什么时候会发生数据写入的丢失：ack参数的配置。</p><p>5、ack模式：取值0，1，all</p><blockquote><blockquote><blockquote></blockquote><pre><code>0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是销量最高。1代表producer往集群发送数据只要leader成功写入消息就可以发送下一条，只确保leader接收成功。-1或all代表producer往集群发送数据需要所有的ISR Follow都完成从Leader的同步才会发送下一条，确保leader发送成功和所有的副本都成功接收。安全性最高，但是效率最低。</code></pre></blockquote></blockquote><p>6、Properties props &#x3D; new Properties(); -&gt;配置生产者客户端参数</p><p>7、props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);-&gt;设置 kafka 集群的地址</p><p>8、props.put(“retries”, 3); -&gt;失败重试次数</p><p>9、props.put(“batch.size”, 10); -&gt;数据发送的批次大小</p><p>10、props.put(“linger.ms”, 10000); -&gt;消息在缓冲区保留的时间，超过设置的值就会被提交到服务端。</p><p>11、props.put(“max.request.size”,10); -&gt;数据发送请求的最大缓存数</p><p>12、props.put(“buffer.memory”, 10240); -&gt;整个 Producer 用到总内存的大小，如果缓冲区满了会提交数据到服务端&#x2F;&#x2F;buffer.memory 要大于 batch.size，否则会报申请内存不足的错误，降低阻塞的可能性。</p><p>13、props.put(“key.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); -&gt;key-value序列化器</p><p>14、props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”); -&gt;字符串最好</p><p>15、在 Kafka 生产者客户端 KatkaProducer 中有3个参数是必填的 -&gt; bootstrap.servers、key.serializer、value.serializer</p><p>16、生产者api参数发送方式（发后即忘）：</p><blockquote><blockquote><blockquote></blockquote><p>   发后即忘，它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下，这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。<br>   ack-&gt;作用在broker<br>   Future<RecordMetadata> send &#x3D; producer.send(rcd);-&gt;也是异步</p></blockquote></blockquote><p>17、生产者api参数发送方式（同步发送）：</p><blockquote><blockquote><blockquote></blockquote><pre><code> producer.send(rcd).get(); -&gt;一旦调用get方法，就会阻塞 Future future = Callable.run()-&gt;有返回值，future.get() Runnable.run()-&gt;无返回值</code></pre></blockquote></blockquote><p>18、生产者api参数发送方式（异步发送）：</p><blockquote><blockquote><blockquote></blockquote><p>   回调函数会在 producer 收到ack时调用，为异步调用，该方法有两个参数，分别是 RecordMetadata 和Exception，如果 Exception 为 null，说明消息发送成功，如果Exception不为 null，说明消息发送失败。<br>   注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p></blockquote></blockquote><p>19、幂等性：生产者将一条数据多次重复写入的情况下，broker端依然只有一条。</p><p>20、在IJ中新建Maven项目，配置pom.xml，重新加载Maven项目<br><img src="/../images/17.png" alt="17"><br><img src="/../images/18.png" alt="18"></p><p>21、新建ProducerDemo类、ProducerCallbackDemo类<br><img src="/../images/19.png" alt="19"><br><img src="/../images/20.png" alt="20"><br><img src="/../images/21.png" alt="21"><br><img src="/../images/22.png" alt="22"></p><p>22、生产者原理<br><img src="/../images/23.png" alt="23"></p><blockquote><blockquote><blockquote><p>（1）、一个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程 。<br>    （2）、在主线程中由 kafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator，也称为消息收集器)中。<br>    （3）、Sender 线程负责从 RecordAccumulator 获取消息并将其发送到 Kafka 中；<br>    （4）、RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。<br>    （5）、RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配置，默认值为 33554432B，即 32M。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 KafkaProducer.send()方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms 的配置，此参数的默认值为 60000，即 60 秒。<br>    （6）、主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列( Deque )中， RecordAccumulator 内部为每个分区都维护了一个双端队列，即 Deque<ProducerBatch>。消息写入缓存时,追加到双端队列的尾部。<br>    （7）、Sender 读取消息时，从双端队列的头部读取。<br>    （8）、注意：ProducerBatch 是指一个消息批次；与此同时，会将较小的 ProducerBatch 凑成一个较大 ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量。<br>    （9）、当topic中的分区数增多的情况下，recordaccumulator中的分区数就会增多。当topic的数量增多的情况下，recordaccumulator中的分区数也会增多。<br>    （10）、ProducerBatch 大小和 batch.size 参数也有着密切的关系。<br>    （11）、当一条消息(ProducerRecord) 流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列(如果没有则新建)，再从这个双端队列的尾部获取一个 ProducerBatch (如果没有则新建)，查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以写入，如果不可以则需要创建一个新的 Producer Batch。<br>    （12）、在新建ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch。如果生产者客户端需要向很多分区发送消息，则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。<br>    （13）、Sender 从 RecordAccumulator 获取缓存的消息之后，会进一步将&lt;分区，Deque<Producer Batch>&gt;的形式转变成&lt;Node,List&lt; ProducerBatch&gt;的形式，其中 Node 表示 Kafka 集群 broker 节点。<br>    （14）、对于网络连接来说，生产者客户端是与具体 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区。<br>    （15）、而对于 KafkaProducer 的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络 I&#x2F;O 层面的转换。<br>    （16）、在转换成&lt;Node, List<ProducerBatch>&gt;的形式之后，Sender 会进一步封装成&lt;Node,Request&gt; 的形式，这样就可以将 Request 请求发往各个 Node了，这里的 Request 是 Kafka 各种协议请求。<br>    （17）、请求在从sender线程发往 Kafka 之前还会保存到InFlightRequests中，InFlightRequests 保存对象的具体形式为 Map&lt;Nodeld, Deque<request>&gt;，它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。<br>    （18）、与此同时，InFlightRequests 还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。<br>    （19）、这个配置参数为max.in.flight.request.per.Connection，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应( Response )。<br>    （20）、通过比较 Deque<Request> 的 size 与这个参数的大小来判断对应的 Node 中是否己经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继其发送请求会增大请求超时的可能。</p></blockquote></blockquote></blockquote><p>23、重要的生产者参数</p><blockquote><blockquote><blockquote></blockquote><p>   （1）、max.request.size<br>这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B，即 1MB 一般情况下，这个默认值就可以满足大多数的应用场景了。<br>这个参数还涉及一些其它参数的联动，比如 broker 端的 message.max.bytes 参数，如果配置错误可能会引起一些不必要的异常；比如将broker 端的message.max.bytes 参数配置为 10，而max.request.size 参数配置为20，那么当发送一条大小为15B的消息时，生产者客户端就会报出异常。<br>   （2）、compression.type<br>这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。该参数还可以配置为”gzip”，”snappy” 和 “lz4”。（服务端也有压缩参数，先解压，再压缩）；对消息进行压缩可以极大地减少网络传输、降低网络 I&#x2F;O，从而提高整体的性能。消息压缩是一种以时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩；没有必要，不需要压缩。<br>   （3）、retries 和 retry.backoff.ms<br>retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、 leader 副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置 retries 大于 0 的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数，那么生产者就会放弃重试并返回异常。重试还和另一个参数retry.backoff.ms有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。<br>Kafka 可以保证同一个分区中的消息是有序的。如果生产者按照一定的顺序发送消息，那么这些消息也会顺序地写入分区，进而消费者也可以按照同样的顺序消费它们。对于某些应用来说，顺序性非常重要，比如 MySQL binlog 的传输，如果出现错误就会造成非常严重的后果；MySQL binlog–&gt;mysql插入数据–&gt;操作结果体会在表中–&gt;mysql为了提高可靠性会把操作记录在日志中–&gt;为了以后的主从同步（mysql集群，主表，子表）–&gt;读写分离–&gt;binlog（mysql自己设计的格式，二进制形式）。<br>如果将 acks 参数配置为非零值，并且max.flight.requests.per.connection 参数配置为大于1的值，那可能会出现错序的现象：如果第一批次消息写入失败，而第二批次消息写入成功，那么生产者会重试发送第一批次的消息，此时如果第一次的消息写入成功，那么这两个批次的消息就出现了错序。<br>一般而言，在需要保证消息顺序的场合建议把参数max.in.flight.requests.per.connection 配置为 1 ，而不是把 acks 配置为0，不过这样也会影响整体的吞吐。–&gt;吞吐量降低<br>   （4）、batch.size<br>每个Batch 要存放 batch.size 大小的数据后，才可以发送出去。比如说 batch.size 默认值是 16KB，那么里面凑够 16KB 的数据才会发送。<br>理论上来说，提升 batch.size 的大小，可以允许更多的数据缓冲在里面，那么一次 Request 发送出去的数据量就更多了，这样吞吐量可能会有所提升。但是 batch.size 也不能过大，要是数据老是缓冲在 Batch 里迟迟不发送出去，那么发送消息的延迟就会很高。一般可以尝试把这个参数调节大些，利用生产环境发消息负载测试一下。<br>   （5）、linger.ms（和batchsize有联系）<br>这个参数用来指定生产者发送 ProducerBatch 之前等待更多消息( ProducerRecord )加入ProducerBatch 时间，默认值为0。生产者客户端会在ProducerBatch 填满或等待时间超过 linger.ms 值时发送出去。增大这个参数的值会增加消息的延迟，但是同时能提升一定的吞吐量。<br>   （6）、enable.idempotence<br>是否开启幂等性功能,详见后续原理加强；幂等性,就是一个操作重复做，每次的结果都一样，x<em>1&#x3D;1，x</em>1&#x3D;1，x*1&#x3D;1；在 kafka 中，就是生产者生产的一条消息，如果多次重复发送，在服务器中的结果还是只有一条。kafka很难实现幂等性，如果重复发，kafka肯定有多条消息–&gt;需要有机制判断曾经是否发送过–&gt;各种手段判断–&gt;事务管理的概念–&gt;加入幂等性，吞吐量会急剧下降。<br>   （7）、partitioner.classes<br>用来指定分区器，默认：org.apache.kafka.internals.DefaultPartitioner –&gt;用hashcode分<br>自定义 partitioner 需要实现 org.apache.kafka.clients.producer.Partitioner 接口 –&gt;可以通过partitioner接口自己实现分区器</p></blockquote></blockquote><p>四、消费者API<br>1、一个正常的消费逻辑需要具备以下几个步骤： </p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、配置消费者客户端参数（2）、创建相应的消费者实例（3）、订阅主题（4）、拉取消息并消费（5）、提交消费位移 offset（6）、关闭消费者实例</code></pre></blockquote></blockquote><p>2、subscribe重载方法：</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、前面两种通过集合的方式订阅一到多个topic</code></pre><p>public void subscribe(Collection<String> topics,ConsumerRebalanceListener listener)<br>public void subscribe(Collection<String> topics)<br>    （2）、后两种主要是采用正则的方式订阅一到多个topic<br>public void subscribe(Pattern pattern, ConsumerRebalanceListener listener)<br>public void subscribe(Pattern pattern)<br>    （3）、正则方式订阅主题（只要是tpc_数字的形式，三位数字以内）<br>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中，如果有人又创建了新的主题，并且主题名字与正表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。利用正则表达式订阅主题，可实现动态订阅；</p></blockquote></blockquote><p>3、assign订阅主题</p><blockquote><blockquote><blockquote></blockquote><pre><code>消费者不仅可以通过 KafkaConsumer.subscribe()方法订阅主题，还可直接订阅某些主题的指定分区； 在KafkaConsumer 中提供了assign()方法来实现这些功能，此方法的具体定义如下：public void assign(Collection&lt;TopicPartition&gt; partitions) ；这个方法只接受参数 partitions，用来指定需要订阅的分区集合。示例如下: consumer.assign(Arrays.asList(new TopicPartition (&quot;tpc_1&quot;,0),new TopicPartition(“tpc_2”,1)))； </code></pre></blockquote></blockquote><p>4、subscribe与assign的区别</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、通过 subscribe()方法订阅主题具有消费者自动再均衡功能；在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。（2）、assign() 方法订阅分区时，是不具备消费者自动均衡的功能的； 其实这一点从 assign()方法参数可以看出端倪,两种类型subscribe()都有ConsumerRebalanceListener类型参数的方法，而assign()方法却没有。</code></pre></blockquote></blockquote><p>5、取消订阅</p><blockquote><blockquote><blockquote></blockquote><pre><code>（1）、可以使用KafkaConsumer中的unsubscribe()方法采取消主题的订阅，这个方法既可以取消通过subscribe( Collection)方式实现的订阅; （2）、也可以取消通过subscribe(Pattem)方式实现的订阅，还可以取消通过 assign(Collection)方式实现的订阅。（3）、如果将subscribe(Collection)或 assign(Collection)集合参数设置为空集合，作用与 unsubscribe()方法相同，如下示例中三行代码的效果相同：consumer.unsubscribe(); consumer.subscribe(new ArrayList&lt;String&gt;()) ; consumer.assign(new ArrayList&lt;TopicPartition&gt;());</code></pre><p>6、消息的消费模式</p><blockquote></blockquote><pre><code>Kafka中的消费是基于拉取模式的。消息的消费一般有两种模式：推送模式和拉取模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用 poll()方法，poll()方法返回的是所订阅的主题(分区)上的一组消息。对于poll () 方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息，那么 poll()方法返回为空的消息集; poll ()方法具体定义如下: public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout) 超时时间参数 timeout，用来控制poll()方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率，可以把timeout设置为Long.MAX_VALUE;消费者消费到的每条消息的类型为 ConsumerRecordtopic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号offset 表示消息在所属分区的偏移量timestamp 表示时间戳，与此对应的timestampType表示时间戳的类型timestampType 有两种类型CreateTime和LogAppendTime，分别代表消息创建的时间戳和消息追加到日志的时间戳headers 表示消息的头部内容key value 分别表示消息的键和消息的值,一般业务应用要读取的就是valueserializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小，如果 key 为空, 则 serializedKeySize 值为-1，同样，如果value 为空，则serializedValueSize的值也会为-1checksum 是 CRC32 的校验值</code></pre></blockquote></blockquote><p>7、指定位移消费</p><blockquote><blockquote><blockquote></blockquote><pre><code>有些时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而KafkaConsumer中的seek()方法正好提供了这个功能，让我们可以追前消费或回溯消费。seek()方法的具体定义如下: public void seek(TopicPartiton partition,long offset)</code></pre></blockquote></blockquote><p>8、再均衡监听器</p><blockquote><blockquote><blockquote></blockquote><pre><code>一个消费组中，一旦有消费者的增减发生，会触发消费者组的 rebalance 再均衡；如果 A 消费者消费掉的一批消息还没来得及提交 offset，而它所负责的分区在 rebalance 中转移给了B消费者，则有可能发生数据的重复消费处理。此情形下，可以通过再均衡监听器做一定程度的补救；</code></pre></blockquote></blockquote><p>9、自动位移提交</p><blockquote><blockquote><blockquote></blockquote><pre><code>Kafka中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数enable.auto.commit 配置，默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置，默认值为 5 秒，此参数生效的前提是 enable.auto.commit 参数为 true。在默认的方式下，消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll()方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。Kafka消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</code></pre></blockquote></blockquote><pre><code>重复消费   &gt;&gt;&gt; 假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。丢失消息   &gt;&gt;&gt; 按照一般思维逻辑而言，自动提交是延时提交，重复消费可以理解，那么消息丢失又是在什么情形下会发生的呢？我们来看下图中的情形：拉取线程不断地拉取消息并存入本地缓存，比如在 BlockingQueue中，另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取，以及第 m 次位移提交的时候，也就是x+6 之前的位移己经确认提交了，处理线程却还正在处理 x+3 的消息；此时如果处理线程发生了异常，待其恢复之后会从第 m 次位移提交处，也就是 x+6 的位置开始拉取消息，那么 x+3 至 x+6 之间的消息就没有得到相应的处理，这样便发生消息丢失的现象。</code></pre><p>10、手动位移提交</p><blockquote><blockquote><blockquote></blockquote><pre><code>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免；同时，自动位移提交也无法做到精确的位移管理。在 Kafka 中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费； 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals，示例如下props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, false); 手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。同步提交的方式：对于采用 commitSync()的无参方法，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的，如果想寻求更细粒度的、更精准的提交，那么就需要使用 commitSync()的另一个有参方法，具体定义如下：public void commitSync(final Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets)异步提交方式：commitSync()方法相反，异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞；可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操。异步提交以便消费者的性能得到一定的增强。</code></pre></blockquote></blockquote><p>11、其他重要参数</p><blockquote><blockquote><blockquote></blockquote><pre><code>fetch.min.bytes=1B -&gt; 一次拉取的最小字节数fetch.max.bytes=50M -&gt; 一次拉取的最大数据量fetch.max.wait.ms=500ms -&gt; 拉取时的最大等待时长max.partition.fetch.bytes = 1MB -&gt; 每个分区一次拉取的最大数据量max.poll.records=500-&gt; 一次拉取的最大条数connections.max.idle.ms=540000ms -&gt; 网络连接的最大闲置时长request.timeout.ms=30000ms  -&gt; 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间metadata.max.age.ms=300000 -&gt; 元数据在限定时间内没有进行更新,则会被强制更新reconnect.backoff.ms=50ms -&gt; 尝试重新连接指定主机之前的退避时间retry.backoff.ms=100ms -&gt; 尝试重新拉取数据的重试间隔</code></pre></blockquote></blockquote><p>12、新建ConsumerDemo、ConsumerDemo1、ConsumerTask、ConsumerDemo2、ConsumerSeekOffset类<br><img src="/../images/24.png" alt="24"><br><img src="/../images/25.png" alt="25"><br><img src="/../images/26.png" alt="26"><br><img src="/../images/27.png" alt="27"><br><img src="/../images/28.png" alt="28"></p><p>五、Topic管理API<br>1、一般情况下，我们都习惯使用 kafka-topic.sh 本来管理主题，如果希望将管理类的功能集成到公司内部的系统中，打造集管理、监控、运维、告警为一体的生态平台，那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类。</p><blockquote><blockquote><blockquote></blockquote><pre><code>KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法：</code></pre><p>   创建主题：CreateTopicResult createTopics(Collection<New Topic> new Topics)<br>   删除主题：DeleteTopicsResult deleteTopics(Collection<String>topics)<br>   列出所有可用的主题：ListTopicsResult listTopics()<br>   查看主题的信息：DescribeTopicsResult describeTopics(Collection<String> topicNames)</p></blockquote></blockquote><p>查询配置信息：</p><blockquote><blockquote><blockquote></blockquote><p>   DescribeConfigsResult describeConfigs(Collection<ConfigResource>resources)<br>   修改配置信息：AlterConfigsResult alterConfigs(Map&lt;ConfigResource,Config&gt; configs)<br>   增加分区：CreatePartitionsResult createPartitions(Map&lt;String,NewPartitions&gt; new Partitions)<br>   构造一个 KafkaAdminClient AdminClient adminClient &#x3D;<br>   KafkaAdminClient.create(props);</p></blockquote></blockquote><p>2、列出主题</p><blockquote><blockquote><blockquote></blockquote><p>   ListTopicsResult listTopicsResult &#x3D; adminClient.listTopics();<br>   Set<String> topics &#x3D; listTopicsResult.names().get();<br>   System.out.println(topics);</p></blockquote></blockquote><p>3、查看主题信息</p><blockquote><blockquote><blockquote></blockquote><p>   DescribeTopicsResult describeTopicsResult &#x3D;<br>    adminClient.describeTopics(Arrays.asList(“tpc_4”, “tpc_3”));<br>   Map&lt;String, TopicDescription&gt; res &#x3D; describeTopicsResult.all().get();<br>   Set<String> ksets &#x3D; res.keySet();<br>   for (String k : ksets) {<br>       System.out.println(res.get(k));<br>   }</p></blockquote></blockquote><p>4、创建主题</p><blockquote><blockquote><blockquote></blockquote><p>   &#x2F;&#x2F; 参数配置<br>   Properties props &#x3D; new Properties();<br>   props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”);<br>   props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000);<br>   &#x2F;&#x2F; 创建 admin client 对象<br>   AdminClient adminClient &#x3D; KafkaAdminClient.create(props);<br>   &#x2F;&#x2F; 由服务端 controller 自行分配分区及副本所在 broker<br>   NewTopic tpc_3 &#x3D; new NewTopic(“tpc_3”, 2, (short) 1);<br>   &#x2F;&#x2F; 手动指定分区及副本的 broker 分配<br>   HashMap&lt;Integer, List<Integer>&gt; replicaAssignments &#x3D; new HashMap&lt;&gt;();<br>   &#x2F;&#x2F; 分区 0,分配到 broker0,broker1<br>    replicaAssignments.put(0,Arrays.asList(0,1));<br>   &#x2F;&#x2F; 分区1,分配到 broker0,broker2<br>   replicaAssignments.put(0,Arrays.asList(0,1));<br>   NewTopic tpc_4 &#x3D; new NewTopic(“tpc_4”, replicaAssignments);<br>   CreateTopicsResult result &#x3D;<br>    adminClient.createTopics(Arrays.asList(tpc_3,tpc_4));<br>   &#x2F;&#x2F; 从 future 中等待服务端返回<br>   try {<br>       result.all().get();<br>   } catch (Exception e) {<br>   e.printStackTrace();<br>   }<br>   adminClient.close();</p></blockquote></blockquote><p>5、删除主题 </p><blockquote><blockquote><blockquote></blockquote><p>   DeleteTopicsResult deleteTopicsResult &#x3D;<br>    adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”));<br>   Map&lt;String, KafkaFuture<Void>&gt; values &#x3D; deleteTopicsResult.values();<br>   System.out.println(values);</p></blockquote></blockquote><p>6、其他管理</p><blockquote><blockquote><blockquote></blockquote><p>   除了进行 topic 管理之外，KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作；</p></blockquote></blockquote><p>7、新建KafkaAdminDemo、CallableDemo类<br><img src="/../images/29.png" alt="29"><br><img src="/../images/30.png" alt="30"><br>**<summary></p><p>**</p></details><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《kafka-API使用方法》&quot;&gt;&lt;a href=&quot;#《kafka-API使用方法》&quot; class=&quot;headerlink&quot; title=&quot;《kafka API使用方法》&quot;&gt;&lt;/a&gt;《kafka API使用方法》&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;title: 《kafka</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>《在kafka集群中部署Eagle运维监控》</title>
    <link href="http://example.com/2022/06/05/Eagle%E3%80%8B/"/>
    <id>http://example.com/2022/06/05/Eagle%E3%80%8B/</id>
    <published>2022-06-05T12:20:41.000Z</published>
    <updated>2022-06-05T12:20:41.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《在kafka集群中部署Eagle运维监控》"><a href="#《在kafka集群中部署Eagle运维监控》" class="headerlink" title="《在kafka集群中部署Eagle运维监控》"></a>《在kafka集群中部署Eagle运维监控》</h1><hr><p>title: 《在kafka集群中部署Eagle运维监控》<br>date: 2022-6-5 20:20:41<br>description: Kafka Eagle </p><details><summary>阅读全文</summary>    kafka eagle（kafka鹰） 是一款由国内公司开源的Kafka集群监控系统，可以用来监视kafka集群的broker状态、Topic信息、IO、内存、consumer线程、偏移量等信息，并进行可视化图表展示。独特的KQL还可以通过SQL在线查询kafka中的数据。<p>1:根据学习通资料下载</p><p>2、解压安装包</p><blockquote><blockquote><blockquote></blockquote><pre><code>[root@node1 ~]#unzip kafka-eagle-bin-2.1.0.tar.gz[root@node1 ~]#unzip kafka-eagle-web-2.0.2-bin.tar.gz</code></pre></blockquote></blockquote><p>3、进入kafka-eagle-web-2.0.2</p><blockquote><blockquote><blockquote></blockquote><pre><code>[root@node1 ~]#cd kafka-eagle-web-2.0.2</code></pre></blockquote></blockquote><p>4、找到目录</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#pwd<br>   &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;kafka-eagle-web</p></blockquote></blockquote><p>5、配置环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   vi &#x2F;etc&#x2F;profile<br>   export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8<br>   export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin<br>   export KE_HOME&#x3D; &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;kafka-eagle-web<br>   export PATH&#x3D;$PATH:$KE_HOME&#x2F;bin</p></blockquote></blockquote><p>6、配置生效</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#. &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>7.通过status查看</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#ke.sh status<br>   [2022-06-05 16:55:05] INFO :kafka Eagle Has Stopped,[563715] .</p></blockquote></blockquote><p>8.配置 KafkaEagle</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#cd &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;kafka-eagle-web&#x2F;conf<br>   [root@node1 conf]#vi system-config.properties</p></blockquote></blockquote><p>需要更改的地方：<br>    &gt;&gt;&gt;<br>   kafka.eagle.zk.cluster.alias&#x3D;cluster1<br>   cluster1.zk.list&#x3D;node1:2181,node2:2181,node3:2181<br>   cluster1.kafka.eagle.broker.size&#x3D;3</p><p>   kafka.eagle.url&#x3D;jdbc:sqlite:&#x2F;export&#x2F;data&#x2F;db&#x2F;ke.db</p><p>9启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录</p><blockquote><blockquote><blockquote><p>[root@node1 ~]mkdir &#x2F;export&#x2F;data&#x2F;db</p></blockquote></blockquote></blockquote><p>10.启动zookeeper</p><blockquote><blockquote><blockquote><p>[root@node1 ~]zkServer.sh start</p></blockquote></blockquote></blockquote><p>11启动kafka</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 ~]cd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;<br>   [root@node1 ~]bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</p></blockquote></blockquote><p>12启动Eagle</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 ~]&#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;bin&#x2F;ke.sh start</p></blockquote></blockquote><p><img src="/../images/31.png" alt="31"><br><img src="/../images/35.jpg" alt="35"><br><img src="/../images/32.jpg" alt="32"></p><p>14、eagle使用功能介绍</p><p>（1）我们还可以直接通过kafka-eagle来发送消息；<br><img src="/../images/36.jpg" alt="36"></p><p>（2）KSQL功能，可以通过SQL语句来查询Topic中的消息；<br><img src="/../images/39.jpg" alt="39"></p><p>（3）可视化工具自然少不了监控，如果你想开启kafka-eagle对Kafka的监控功能的话，需要修改Kafka的启动脚本，暴露JMX的端口；</p><blockquote><blockquote><blockquote></blockquote><p>   [root@node1 kafka-eagle-web-2.0.2 ]#vi kafka-server-start.sh</p></blockquote></blockquote><h1 id="暴露JMX端口"><a href="#暴露JMX端口" class="headerlink" title="暴露JMX端口"></a>暴露JMX端口</h1><p>   if [ “x$KAFKA_HEAP_OPTS” &#x3D; “x” ]; then<br>       export KAFKA_HEAP_OPTS&#x3D;”-server -Xms2G -Xmx2G -XX:PermSize&#x3D;128m -XX:+UseG1GC -XX:MaxGCPauseMillis&#x3D;200 -XX:ParallelGCThreads&#x3D;8 -XX:ConcGCThreads&#x3D;5 -XX:InitiatingHeapOccupancyPercent&#x3D;70”<br>       export JMX_PORT&#x3D;”9999”<br>   fi </p><p>（4）监控图表界面；如下：<br><img src="/../images/37.jpg" alt="37"></p><p>（5）监控大屏功能；<br><img src="/../images/38.jpg" alt="38"></p><p>**</p></details><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《在kafka集群中部署Eagle运维监控》&quot;&gt;&lt;a href=&quot;#《在kafka集群中部署Eagle运维监控》&quot; class=&quot;headerlink&quot; title=&quot;《在kafka集群中部署Eagle运维监控》&quot;&gt;&lt;/a&gt;《在kafka集群中部署Eagle运维监</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/05/28/hello-world/"/>
    <id>http://example.com/2022/05/28/hello-world/</id>
    <published>2022-05-28T09:50:41.659Z</published>
    <updated>2022-05-28T09:50:41.659Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>《Spark local&amp; stand-alone配置》</title>
    <link href="http://example.com/2022/05/22/spark1/"/>
    <id>http://example.com/2022/05/22/spark1/</id>
    <published>2022-05-22T08:18:57.000Z</published>
    <updated>2022-05-22T08:18:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spark-Standalone-HA模式<br>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在<br>着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节<br>点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下<br>master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。<br>此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下<br>载配置新的版本的 zookeeper<br>配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>在 master 节点上重新进行前面配置的 zookeeper 操作</p><blockquote><blockquote><blockquote></blockquote><p>   1.上传apache-zookeeper-3.7.0-bin.tar.gz 到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 2.在 &#x2F;export&#x2F;server 目录下创建软连接 3.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<br>    vim spark-env.sh</p></blockquote></blockquote><p>为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都<br>可以做 master</p><blockquote><blockquote><blockquote></blockquote><p>   结果显示：<br>     ……<br>      82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST&#x3D;master<br>     ………</p></blockquote></blockquote><p>文末添加内容</p><blockquote><blockquote><blockquote><p>SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -<br>       Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha”<br>        #spark.deploy.recoveryMode<br>        指定HA模式 基于Zookeeper实现<br>        #指定Zookeeper的连接地址<br>        #指定在Zookeeper中注册临时节点的路径</p></blockquote></blockquote></blockquote><p>分发 spark-env.sh 到 salve1 和 slave2 上</p><blockquote><blockquote><blockquote></blockquote><pre><code>scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/</code></pre></blockquote></blockquote><p>启动之前确保 Zookeeper 和 HDFS 均已经启动<br>启动集群:</p><blockquote><blockquote><blockquote></blockquote><p>   #在 master 上 启动一个master 和全部worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh<br>    结果显示：<br>    (base) [root@master ~]# jps<br>    37328 DataNode<br>    41589 Master<br>    35798 QuorumPeerMain<br>    38521 ResourceManager<br>    46281 Jps<br>    38907 NodeManager<br>    41821 Worker<br>    36958 NameNode (base)<br>    [root@slave1 sbin]# jps<br>    36631 DataNode<br>    48135 Master<br>    35385 QuorumPeerMain<br>    37961 NodeManager<br>    40970 Worker<br>    48282 Jps<br>    37276 SecondaryNameNode</p></blockquote></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><blockquote></blockquote><pre><code>http://master:8081/</code></pre><p>   <a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote><p>此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><blockquote><blockquote><blockquote></blockquote><p>   #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p></blockquote></blockquote><p>访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>进行主备切换的测试<br>提交一个 spark 任务到当前 活跃的 master上 :</p><blockquote><blockquote><blockquote><p>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000</p></blockquote></blockquote></blockquote><p>复制标签 kill 掉 master 的 进程号<br>再次访问 master 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://master:8081/">http://master:8081/</a><br>      网页访问不了！</p></blockquote></blockquote></blockquote><p>再次访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>可以看到当前活跃的 master 提示信息</p><blockquote><blockquote><blockquote><p>(base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]#</p></blockquote></blockquote></blockquote><p>Spark On YARN模式</p><blockquote><blockquote><blockquote><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无</p></blockquote></blockquote></blockquote><p>需部署Spark集群, 只要找一台服务器, 充当Spark的客户端<br>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><blockquote><blockquote><blockquote><p>spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ….</p></blockquote></blockquote></blockquote><p>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）<br>bin&#x2F;pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下<br> bin&#x2F;spark-shell –master yarn –deploy-mode client|cluster<br>bin&#x2F;spark-submit –master yarn –deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py 参数</p><blockquote><blockquote><blockquote></blockquote><p>  spark-submit 和 spark-shell 和 pyspark的相关参数</p></blockquote></blockquote><ul><li>bin&#x2F;pyspark: pyspark解释器spark环境 - bin&#x2F;spark-shell: scala解释器spark环境 - bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具 - bin&#x2F;spark-sql: sparksql客户端工具<br>  这4个客户端工具的参数基本通用.以spark-submit 为例: bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 xxx.py&#96;<blockquote><blockquote><blockquote></blockquote><p>  Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>   Usage: spark-submit –kill [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit –status [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit run-example [options] example-class [example args] </p><blockquote></blockquote><p>   Options: –master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;<a href="https://host:port">https://host:port</a>, or </p><blockquote></blockquote><p>   local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java &#x2F; Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the </p><blockquote></blockquote><p>   driver and executor classpaths. –packages Comma-separated list of maven coordinates of </p><blockquote></blockquote><p>   jars to include on the driver and executor classpaths. Will </p><blockquote></blockquote><p>   search the local maven repo, then maven central and any </p><blockquote></blockquote><p>   additional remote repositories given by –repositories. The </p><blockquote></blockquote><p>   format for the coordinates should be </p><blockquote></blockquote><p>   groupId:artifactId:version.<br>   –exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br> – packages.<br> –py-files PY_FILES 指定Python程序依赖的其它python文件<br> –files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>  –archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br> –conf,<br> -c PROP&#x3D;VALUE 手动指定配置<br> –properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.<br> –executor-memory MEM Executor的内存 (Default: 1G).<br> –proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br> –principal &#x2F;<br> –keytab.<br> –help,<br> -h 显示帮助文件<br>  –verbose,<br>  -v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):<br>   –driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>   –supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>   –kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:<br>   –total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:<br>    –executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):<br>    –num-executors NUM Executor应该开启几个<br>    –principal PRINCIPAL Principal to be used to login to KDC.<br>    –keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>    –queue QUEUE_NAME 指定运行的YARN队列(Default: “default”)</p></blockquote></blockquote></li></ul><p>启动 YARN 的历史服务器</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver</p></blockquote></blockquote></blockquote><p>访问WebUI界面</p><blockquote><blockquote><blockquote><p><a href="http://master:19888/">http://master:19888/</a></p></blockquote></blockquote></blockquote><p>client 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py</p></blockquote></blockquote></blockquote><p>cluster 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**</p></blockquote></blockquote></blockquote></details><hr><p>title: ‘《Spark local&amp; stand-alone配置》’<br>toc: false<br>comments: true<br>keywords: ‘’<br>description: ‘’<br>date: 2022-05-22 16:18:57<br>updated: 2022-05-22 16:18:57<br>categories:<br>tags:<br>top:<br>academia: true</p><hr><h1 id="《Spark-local-amp-stand-alone配置》"><a href="#《Spark-local-amp-stand-alone配置》" class="headerlink" title="《Spark local&amp; stand-alone配置》"></a>《Spark local&amp; stand-alone配置》</h1><details><summary>阅读全文</summary><p>**<summary>本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境<br>Anaconda On Linux 安装 (单台服务器脚本安装)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装<br>位置在 &#x2F;export&#x2F;server:</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server<br>   #运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh<br>   过程显示：<br>   …<br>   #出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] </p><blockquote></blockquote><p>   &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p></blockquote></blockquote><p>安装完成后, 退出终端， 重新进来:</p><blockquote><blockquote><blockquote><p>exit<br>   结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)<br>   [root@node1 ~]#</p></blockquote></blockquote></blockquote><p>创建虚拟环境 pyspark 基于 python3.8</p><blockquote><blockquote><blockquote></blockquote><p>   conda create -n pyspark python&#x3D;3.8</p></blockquote></blockquote><p>切换到虚拟环境内</p><blockquote><blockquote><blockquote></blockquote><p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]#</p></blockquote></blockquote><p>在虚拟环境内安装包 （有WARNING不用管）</p><blockquote><blockquote><blockquote></blockquote><p>   pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p></blockquote></blockquote><p>spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;</p></blockquote></blockquote><p>建立软连接</p><blockquote><blockquote><blockquote></blockquote><p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p></blockquote></blockquote><p>添加环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   SPARK_HOME: 表示Spark安装路径在哪里<br>   PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器<br>   JAVA_HOME: 告知Spark Java在哪里<br>   HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里<br>   HADOOP_HOME: 告知Spark Hadoop安装在哪里</p></blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile<br>   内容：<br>   …..<br>   注：此部分之前配置过，此部分不需要在配置<br>   #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar </p><p>   #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin </p><p>   #ZOOKEEPER_HOME export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin ….. </p><p>   #将以下部分添加进去 #SPARK_HOME export SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python<br>   vim .bashrc<br>   内容添加进去： </p><p>   #JAVA_HOME<br>   export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241<br>   #PYSPARK_PYTHON<br>   export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python</p><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile<br>   source ~&#x2F;.bashrc</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;</p></blockquote></blockquote><p>开启 </p><blockquote><blockquote><blockquote></blockquote><p>  .&#x2F;pyspark 结果显示： (base) [root@master bin]# .&#x2F;pyspark<br>   Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux<br>   Type “help”, “copyright”, “credits” or “license” for more information.<br>   Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicable<br>    Welcome to<br>         __              __<br>    __ &#x2F; <strong>&#x2F;</strong> ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>     <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F; ‘</em>&#x2F;<br>     &#x2F;_</em> &#x2F; .</em><em>&#x2F;_,</em>&#x2F;</em>&#x2F; &#x2F;<em>&#x2F;_\ version 3.2.0<br>        &#x2F;</em>&#x2F;<br>    Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at <a href="http://master:4040/">http://master:4040</a> Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt;</p></blockquote></blockquote><p>查看WebUI界面</p><blockquote><blockquote><blockquote><p>浏览器访问：<br>    <a href="http://node1:4040/">http://node1:4040/</a></p></blockquote></blockquote></blockquote><p>退出</p><blockquote><blockquote><blockquote><p>conda deactivate</p></blockquote></blockquote></blockquote><p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server:</p><p>cd &#x2F;export&#x2F;server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh</p><blockquote><blockquote><blockquote><p>过程显示：<br> …<br> #出现内容选 yes<br>  Please answer ‘yes’ or ‘no’:’<br>yes<br>   …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p></blockquote></blockquote></blockquote><p>安装完成后, 退出终端，</p><blockquote><blockquote><blockquote></blockquote><p>  重新进来:<br>  exit<br>  结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境.<br>    Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1<br>   …</p></blockquote></blockquote><p>在 master 节点上把 .&#x2F;bashrc 和 profile 分发给 slave1 和 slave2</p><blockquote><blockquote><blockquote></blockquote><p>   #分发 .bashrc : scp <del>&#x2F;.bashrc root@slave1:</del>&#x2F; scp <del>&#x2F;.bashrc root@slave2:</del>&#x2F; #分发 profile : scp &#x2F;etc&#x2F;profile&#x2F; root@slave1:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@slave2:&#x2F;etc&#x2F;<br>   …</p></blockquote></blockquote><p>创建虚拟环境 pyspark 基于 python3.8</p><blockquote><blockquote><blockquote></blockquote><p>   conda create -n pyspark python&#x3D;3.8</p></blockquote></blockquote><p>切换到虚拟环境内</p><blockquote><blockquote><blockquote></blockquote><p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)<br>在虚拟环境内安装包 （有WARNING不用管）<br>    pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>    spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p></blockquote></blockquote><p>master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</p></blockquote></blockquote><p>将文件 workers.template 改名为 workers，并配置文件内容</p><blockquote><blockquote><blockquote></blockquote><p>   mv workers.template workers vim workers<br>    # localhost删除，内容追加文末： node1<br>    node2<br>    node3<br>    # 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</p></blockquote></blockquote><p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p><blockquote><blockquote><blockquote></blockquote><p>   mv spark-env.sh.template spark-env.sh vim spark-env.sh</p></blockquote></blockquote><blockquote><blockquote><blockquote><p>文末追加内容：<br>   ##设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk<br>   ##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;master<br>    #告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077<br>    # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080<br>    # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1<br>    # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g<br>    # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078<br>    # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081<br>    ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;”- Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; - Dspark.history.fs.cleaner.enabled&#x3D;true”</p></blockquote></blockquote></blockquote><p>开启 hadoop 的 hdfs 和 yarn 集群</p><blockquote><blockquote><blockquote></blockquote><pre><code>start-dfs.sh start-yarn.sh</code></pre><p>在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下:</p><blockquote></blockquote><p>   hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog</p></blockquote></blockquote><p>将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</p><blockquote><blockquote><blockquote></blockquote><p>   mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;master:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true</p></blockquote></blockquote><p>配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为<br>log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为<br>WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</p><blockquote><blockquote><blockquote></blockquote><p>   mv log4j.properties.template log4j.properties vim log4j.properties 结果显示：<br>    …<br>    18 # Set everything to be logged to the console<br>    19 log4j.rootCategory&#x3D;WARN, console ….</p></blockquote></blockquote><p>master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</p><blockquote><blockquote><blockquote></blockquote><p>   master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</p></blockquote></blockquote><p>在slave1 和 slave2 上做软连接</p><blockquote><blockquote><blockquote></blockquote><p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p></blockquote></blockquote><p>重新加载环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh</p></blockquote></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><blockquote></blockquote><p>   浏览器访问： <a href="http://master:18080/">http://master:18080/</a></p></blockquote></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Spark-Standalone-HA模式&lt;br&gt;Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在&lt;br&gt;着Master 单点故障(SPOF)的问题。简单理解为，spark-Standal</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>《spark基础环境配置》</title>
    <link href="http://example.com/2022/05/22/spark2/"/>
    <id>http://example.com/2022/05/22/spark2/</id>
    <published>2022-05-22T08:18:57.000Z</published>
    <updated>2022-05-22T08:18:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《spark基础环境配置》"><a href="#《spark基础环境配置》" class="headerlink" title="《spark基础环境配置》"></a>《spark基础环境配置》</h1><p>打开一个hosts映射文件,为了保证后续相互关联的虚拟机能够通过主机名进行访问，根据实际需求配置<br>对应的IP和主机名映射，分别将主机名master、slave1、slave2 与IP地址 192.168.88.134、<br>192.168.88.135 和192.168.88.136进行了匹配映射(这里通常要根据实际需要，将要搭建的集群主机都<br>配置主机名和IP映射)。<br>编辑 &#x2F;etc&#x2F;hosts 文件</p><blockquote><blockquote><blockquote><p>vim &#x2F;etc&#x2F;hosts<br>   内容修改为（注：三台主机内容一样）<br>   localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6<br>   192.168.88.135 node1<br>   192.168.88.136 node2<br>   192.168.88.137 node3</p></blockquote></blockquote></blockquote><p>三、集群配置时间同步<br>定义：网络时间服务协议（Network Time Protocol, NTP），是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器做时间同步化。<br>原因：时间同步服务器，顾名思义就是来同步时间的。在集群中同步时间有着十分重要的作用，负载均衡集群或高可用集群如果时间不一致，在服务器之间的数据误差就会很大，寻找数据便会成为一件棘手的事情。若是时间无法同步，那么就算是备份了数据，你也可能无法在正确的时间将正确的数据备份。那损失可就大了。<br>yum 安装 ntp （注：三台主机做同样操作）</p><blockquote><blockquote><blockquote><p>yum install ntp -y<br>   开机自启动ntp<br>   systemctl enable ntpd &amp;&amp; systemctl start ntpd<br>   结果显示： [root@master ~]# systemctl enable ntpd &amp;&amp; systemctl start ntpd Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi- user.target.wants&#x2F;ntpd.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ntpd.service.</p></blockquote></blockquote></blockquote><p>授权 192.168.88.0-192.168.10.255 网段上的所有机器可以从这台机器上查询和同步时间</p><blockquote><blockquote><blockquote></blockquote><p>   #查看ntp配置文件<br>   ls -al &#x2F;etc | grep ‘ntp’<br>   #显示内容<br>   [root@node1 etc]# ls -al &#x2F;etc | grep ‘ntp’<br>   drwxr-xr-x 3 root root 52 3月 10 18:25 ntp<br>   -rw-r–r– 1 root root 2041 3月 10 20:03 ntp.conf<br>    #编辑内容添加 restrict 192.168.88.0 mask 255.255.255.0 （注：在17行左右） vim &#x2F;etc&#x2F;ntp.conf<br>    16 # Hosts on local network are less restricted.<br>    17 restrict 192.168.88.0 mask 255.255.255.0</p></blockquote></blockquote><p>集群在局域网中，不使用其他互联网上的时间</p><blockquote><blockquote><blockquote></blockquote><pre><code>#修改 /etc/ntpd.conf 内容vim vim /etc/ntp.conf # </code></pre></blockquote></blockquote><pre><code>将21-24行内容注释掉（注：原来未注释）   21 #server 0.centos.pool.ntp.org iburst 22 #server 1.centos.pool.ntp.org iburst 23 #server 2.centos.pool.ntp.org iburst 24 #server 3.centos.pool.ntp.org iburst # 在25行添加 server masterIP 即为： server 192.168.88.135</code></pre><p>node 和 node3 相同操作<br>三台主机同时执行</p><blockquote><blockquote><blockquote></blockquote><p>   systemctl enable ntpd &amp;&amp; systemctl start ntpd</p></blockquote></blockquote><p>查看ntp端口</p><blockquote><blockquote><blockquote></blockquote><p>   [root@master etc]# ss -tupln | grep ‘123’ udp UNCONN 0 0 192.168.88.135:123 <em>:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;19)) udp UNCONN 0 0 127.0.0.1:123 <em>:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;18)) udp UNCONN 0 0 <em>:123 <em>:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;16)) udp UNCONN 0 0 [fe80::2832:5f98:5bc0:e621]%ens33:123 [::]:</em> users:((“ntpd”,pid&#x3D;54823,fd&#x3D;23)) udp UNCONN 0 0 [::1]:123 [::]:* users:((“ntpd”,pid&#x3D;54823,fd&#x3D;20)) udp UNCONN 0 0 [::]:123 [::]:* users:((“ntpd”,pid&#x3D;54823,fd&#x3D;17))</p></blockquote></blockquote><p>配置完成后三台主机都需要重启</p><blockquote><blockquote><blockquote></blockquote><p>   shutdown -r 0</p></blockquote></blockquote><p>三台主机同时执行（注：此过程需要5分钟左右）</p><blockquote><blockquote><blockquote></blockquote><p>   ntpstat</p></blockquote></blockquote><p>三、ssh免密钥登陆<br>SSH免密钥登陆可以更加方便的实现不同计算机之间的连接和切换<br>master 生成公钥私钥 (一路回车)<br>ssh-keygen </p><blockquote><blockquote><blockquote></blockquote><p>   #结果显示：<br>    [root@master .ssh]# ssh-keygen Generating public&#x2F;private rsa key pair.<br>    Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa):<br>    Enter passphrase (empty for no passphrase):<br>    Enter same passphrase again:<br>    Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.<br>    Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub. T<br>    he key fingerprint is: SHA256:QUAgFH5KBc&#x2F;Erlf1JWSBbKeEepPJqMBqpWbc02&#x2F;uFj8 root@master The key’s randomart image is:<br>    +—[RSA 2048]—-+<br>    | .&#x3D;++oo+.o+.     |<br>    | . <em>. ..</em>.o .    |<br>    |. o.++ *.+ o     |</p></blockquote></blockquote><pre><code>|.o ++ B ...      | |o.=o.o .S        ||.*oo.. .         | |+ .. . o         | | + E             | | =o .            | +----[SHA256]-----+</code></pre><p>查看隐藏的 .ssh 文件</p><blockquote><blockquote><blockquote></blockquote><p>   la -al .ssh </p></blockquote></blockquote><pre><code># 结果显示 [root@master ~]# ls -al .ssh/ 总用量 16 drwx------ 2 root root 80 3月 10 21:52 . dr-xr-x---. 4 root root 175 3月 10 21:45 .. -rw------- 1 root root 393 3月 10 21:52 authorized_keys -rw------- 1 root root 1675 3月 10 21:48 id_rsa -rw-r--r-- 1 root root 393 3月 10 21:48 id_rsa.pub -rw-r--r-- 1 root root 366 3月 10 21:54 known_hosts</code></pre><p>master 配置免密登录到master slave1 slave2</p><blockquote><blockquote><blockquote></blockquote><p>   ssh-copy-id master<br>    ssh-copy-id slave1<br>    ssh-copy-id slave2</p></blockquote></blockquote><p>四、安装配置 jdk<br>编译环境软件安装目录</p><blockquote><blockquote><blockquote></blockquote><p>   mkdir -p &#x2F;export&#x2F;server</p></blockquote></blockquote><p>JDK 1.8安装 上传 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</p><blockquote><blockquote><blockquote></blockquote><p>   tar -zxvf jdk-8u241-linux-x64.tar.gz</p></blockquote></blockquote><p>配置环境变量</p><blockquote><blockquote><blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar</p></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>查看 java 版本号</p><blockquote><blockquote><blockquote><p>java -version 结果显示： [root@master jdk1.8.0_241]# java -version java version “1.8.0_241” Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</p></blockquote></blockquote></blockquote><p>master 节点将 java 传输到 slave1 和 slave2</p><blockquote><blockquote><blockquote></blockquote><p>   scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave1:&#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@slave2:&#x2F;export&#x2F;server&#x2F;</p></blockquote></blockquote><p>配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）<br>在 master slave1 和slave2 创建软连接</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server<br>    ln -s jdk1.8.0_241&#x2F; jdk</p></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><p>zookeeper安装配置<br>配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 master.root slave1.root slave2.root</p><blockquote><blockquote><blockquote></blockquote><pre><code>vim /etc/hosts #结果显示 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 </code></pre></blockquote></blockquote><pre><code>192.168.88.135 master master.root 192.168.88.136 slave1 slave1.root 192.168.88.137 slave2 slave2.root</code></pre><p>zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F; tar -zxvf zookeeper-3.4.10.tar.gz</p></blockquote></blockquote><p>在 &#x2F;export&#x2F;server 目录下创建软连接</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server ln -s zookeeper-3.4.10&#x2F; zookeeper</p></blockquote></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; cp zoo_sample.cfg zoo.cfg</p></blockquote></blockquote></blockquote><p>接上步给 zoo.cfg 添加内容</p><blockquote><blockquote><blockquote><p>#Zookeeper的数据存放目录 dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas<br>    # 保留多少个快照 autopurge.snapRetainCount&#x3D;3<br>    # 日志多少小时清理一次 autopurge.purgeInterval&#x3D;1<br>    # 集群中服务器地址 server.1&#x3D;master:2888:3888 server.2&#x3D;slave1:2888:3888 server.3&#x3D;slave2:2888:3888</p></blockquote></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</p><blockquote><blockquote><blockquote><p> cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdata touch myid echo ‘1’ &gt; myid</p></blockquote></blockquote></blockquote><p>将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给slave1 和 slave2</p><blockquote><blockquote><blockquote><p>scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD</p></blockquote></blockquote></blockquote><p>推送成功后，分别在 slave1 和 slave2 上创建软连接</p><blockquote><blockquote><blockquote><p>ln -s zookeeper-3.4.10&#x2F; zookeeper</p></blockquote></blockquote></blockquote><p>接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid中的内容分别改为 2 和3</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 结果显示： [root@slave1 zkdatas]# vim myid [root@slave1 zkdatas]# more myid 2[root@slave2 zkdatas]# vim myid [root@slave2 zkdatas]# more myid 3</p></blockquote></blockquote><p>配置zookeeper的环境变量（注：三台主机都需要配置）</p><blockquote><blockquote><blockquote><p> vim &#x2F;etc&#x2F;profile # zookeeper 环境变量 export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin</p></blockquote></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote><p>source &#x2F;etc&#x2F;profile</p></blockquote></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做）</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin zkServer.sh start<br>    结果显示： [root@master bin]# .&#x2F;zkServer.sh start ZooKeeper JMX enabled by default Using config: &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg Starting zookeeper … STARTED</p></blockquote></blockquote></blockquote><p>zookeeper 的状态</p><blockquote><blockquote><blockquote></blockquote><pre><code>zkServer.sh status 结果显示： [root@master server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave1 server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave2 conf]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader</code></pre></blockquote></blockquote><p>jps 结果显示： </p><blockquote><blockquote><blockquote></blockquote><p>   [root@master server]# jps 125348 QuorumPeerMain 16311 Jps [root@slave1 server]# jps 126688 QuorumPeerMain 17685 Jps [root@slave2 conf]# jps 126733 QuorumPeerMain 17727 Jps</p></blockquote></blockquote><p>脚本一键启动</p><blockquote><blockquote><blockquote></blockquote><p>   vim zkServer.sh<br>  #!&#x2F;bin&#x2F;bash<br>  if [ $# -eq 0 ] ;<br>  then<br>       echo “please input param:start stop”<br>else<br>if [ $1 &#x3D; start ] ;then<br>   echo “${1}ing master”<br>   ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start”<br>         for i in {1..2}<br>          do<br>             echo “${1}ping slave${i}” </p></blockquote></blockquote><pre><code>         ssh slave$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;     done </code></pre><p>fi<br>if [ $1 &#x3D; stop ];then<br>    echo “${1}ping master “<br>    ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop”<br>    for i in {1..2}<br>    do<br>       echo “${1}ping slave${i}” ssh slave${i} “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop”<br>    done<br>fi<br>if [ $1 &#x3D; status ];then<br>    echo “${1}ing master”<br>    ssh master “source &#x2F;etc&#x2F;profile;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status”<br>    for i in {1..2}<br>    do<br>       echo “${1}ping slave${i}”<br>        ssh slave${i} “source &#x2F;etc&#x2F;profile;<br>&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status”<br>   done<br> fi<br> fi<br> #将文件放在 &#x2F;bin 目录下 chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh</p><p>Hadoop 安装配置<br>把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件</p><blockquote><blockquote><blockquote><p>tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</p></blockquote></blockquote></blockquote><pre><code>修改配置文件(进入路径 /export/server/hadoop-3.3.0/etc/hadoop)cd /export/server/hadoop-3.3.0/etc/hadoophadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=rootcore-site.xml&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 - -&gt;&lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://master:8020&lt;/value&gt;          &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt;     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;     &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt;      &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;      &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; </code></pre><!-- 整合hive 用户代理设置 --><property>     <name>hadoop.proxyuser.root.hosts</name>     <value>*</value>     </property>     <property>      <name>hadoop.proxyuser.root.groups</name>      <value>*</value>      </property> <     !-- 文件系统垃圾桶保存时间 --> <property>      <name>fs.trash.interval</name>      <value>1440</value>     </property>    hdfs-site.xml    <!-- 设置SNN进程运行机器位置信息 -->     <property>     <name>dfs.namenode.secondary.http-address</name> <value>slave1:9868</value>     </property>    mapred-site.xml    <!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --> <property>     <name>mapreduce.framework.name</name>     <value>yarn</value>     </property>     <!-- MR程序历史服务地址 -->     <property>     <name>mapreduce.jobhistory.address</name>     <value>master:10020</value>     </property>     <!-- MR程序历史服务器web端地址 -->     <property>     <name>mapreduce.jobhistory.webapp.address</name> <value>master:19888</value>     </property>     <property>     <name>yarn.app.mapreduce.am.env</name>     <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>     </property>     <property>     <name>mapreduce.map.env</name>     <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>     </property>     <property>     <name>mapreduce.reduce.env</name>    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>     </property>yarn-site.xml<!-- 设置YARN集群主角色运行机器位置 --> <property> <name>yarn.resourcemanager.hostname</name> <value>master</value> </property> <property> <name>yarn.nodemanager.aux-services</name>  <value>mapreduce_shuffle</value>  </property>  <!-- 是否将对容器实施物理内存限制 -->   <property>   <name>yarn.nodemanager.pmem-check-enabled</name>    <value>false</value>    </property>     <!-- 是否将对容器实施虚拟内存限制。 -->      <property>      <name>yarn.nodemanager.vmem-check-enabled</name>      <value>false</value>       </property>       <!-- 开启日志聚集 -->        <property> <name>yarn.log-aggregation-enable</name> <value>true</value>        </property>        <!-- 设置yarn历史服务器地址 -->         <property>         <name>yarn.log.server.url</name>         <value>http://master:19888/jobhistory/logs</value>         </property>         <!-- 历史日志保存的时间 7天 -->         <property>         <name>yarn.log-aggregation.retain-seconds</name> <value>604800</value>         </property><pre><code>     node1    node2      node3</code></pre><p>分发同步hadoop安装包</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server<br>     scp -r hadoop-3.3.0 root@slave1:$PWD<br>     scp -r hadoop-3.3.0 root@slave2:$PWD<br>将hadoop添加到环境变量</p><blockquote></blockquote><p>   vim &#x2F;etc&#x2F;profile export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin</p></blockquote></blockquote><p>重新加载环境变量文件</p><blockquote><blockquote><blockquote></blockquote><p>   source &#x2F;etc&#x2F;profile</p></blockquote></blockquote><pre><code>Hadoop集群启动格式化namenode（只有首次启动需要格式化）hdfs namenode -format</code></pre><p>脚本一键启动<br>    [root@master ~]# start-dfs.sh Starting namenodes on [master] 上一次登录：五 3月 11 21:27:24 CST 2022pts&#x2F;0 上 Starting datanodes 上一次登录：五 3月 11 21:27:32 CST 2022pts&#x2F;0 上 Starting secondary namenodes [slave1] 上一次登录：五 3月 11 21:27:35 CST 2022pts&#x2F;0 上 </p><pre><code>[root@master ~]# start-yarn.sh Starting resourcemanager 上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上 Starting nodemanagers 上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上启动后 输入 jps 查看[root@master ~]# jps 127729 NameNode 127937 DataNode 14105 Jps 128812 NodeManager 128591 ResourceManager [root@slave1 hadoop]# jps 121889 NodeManager 121559 SecondaryNameNode 7014 Jps 121369 DataNode [root@slave2 hadoop]# jps 6673 Jps 121543 NodeManager 121098 DataNode</code></pre><p>WEB页面<br>HDFS集群：</p><blockquote><blockquote><blockquote></blockquote><pre><code>http://master:9870/</code></pre></blockquote></blockquote><p>YARN集群：</p><blockquote><blockquote><blockquote><p><a href="http://master:9870/">http://master:9870/</a></p></blockquote></blockquote></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《spark基础环境配置》&quot;&gt;&lt;a href=&quot;#《spark基础环境配置》&quot; class=&quot;headerlink&quot; title=&quot;《spark基础环境配置》&quot;&gt;&lt;/a&gt;《spark基础环境配置》&lt;/h1&gt;&lt;p&gt;打开一个hosts映射文件,为了保证后续相互关联的虚</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>《Spark HA &amp; Yarn配置》</title>
    <link href="http://example.com/2022/05/22/spark3/"/>
    <id>http://example.com/2022/05/22/spark3/</id>
    <published>2022-05-22T08:18:57.000Z</published>
    <updated>2022-05-22T08:18:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《Spark-HA-amp-Yarn配置》"><a href="#《Spark-HA-amp-Yarn配置》" class="headerlink" title="《Spark HA &amp; Yarn配置》"></a>《Spark HA &amp; Yarn配置》</h1><p>Spark-Standalone-HA模式<br>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在<br>着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节<br>点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下<br>master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。<br>此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下<br>载配置新的版本的 zookeeper<br>配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>在 master 节点上重新进行前面配置的 zookeeper 操作</p><blockquote><blockquote><blockquote></blockquote><p>   1.上传apache-zookeeper-3.7.0-bin.tar.gz 到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件 2.在 &#x2F;export&#x2F;server 目录下创建软连接 3.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处</p></blockquote></blockquote><p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容</p><blockquote><blockquote><blockquote></blockquote><p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<br>    vim spark-env.sh</p></blockquote></blockquote><p>为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都<br>可以做 master</p><blockquote><blockquote><blockquote></blockquote><p>   结果显示：<br>     ……<br>      82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST&#x3D;master<br>     ………</p></blockquote></blockquote><p>文末添加内容</p><blockquote><blockquote><blockquote><p>SPARK_DAEMON_JAVA_OPTS&#x3D;”-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -<br>       Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha”<br>        #spark.deploy.recoveryMode<br>        指定HA模式 基于Zookeeper实现<br>        #指定Zookeeper的连接地址<br>        #指定在Zookeeper中注册临时节点的路径</p></blockquote></blockquote></blockquote><p>分发 spark-env.sh 到 salve1 和 slave2 上</p><blockquote><blockquote><blockquote></blockquote><pre><code>scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/</code></pre></blockquote></blockquote><p>启动之前确保 Zookeeper 和 HDFS 均已经启动<br>启动集群:</p><blockquote><blockquote><blockquote></blockquote><p>   #在 master 上 启动一个master 和全部worker &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-master.sh<br>    结果显示：<br>    (base) [root@master ~]# jps<br>    37328 DataNode<br>    41589 Master<br>    35798 QuorumPeerMain<br>    38521 ResourceManager<br>    46281 Jps<br>    38907 NodeManager<br>    41821 Worker<br>    36958 NameNode (base)<br>    [root@slave1 sbin]# jps<br>    36631 DataNode<br>    48135 Master<br>    35385 QuorumPeerMain<br>    37961 NodeManager<br>    40970 Worker<br>    48282 Jps<br>    37276 SecondaryNameNode</p></blockquote></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><blockquote></blockquote><pre><code>http://master:8081/</code></pre><p>   <a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote><p>此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><blockquote><blockquote><blockquote></blockquote><p>   #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p></blockquote></blockquote><p>访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>进行主备切换的测试<br>提交一个 spark 任务到当前 活跃的 master上 :</p><blockquote><blockquote><blockquote><p>&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000</p></blockquote></blockquote></blockquote><p>复制标签 kill 掉 master 的 进程号<br>再次访问 master 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://master:8081/">http://master:8081/</a><br>      网页访问不了！</p></blockquote></blockquote></blockquote><p>再次访问 slave1 的 WebUI</p><blockquote><blockquote><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote></blockquote><p>可以看到当前活跃的 master 提示信息</p><blockquote><blockquote><blockquote><p>(base) [root@master ~]# &#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 22&#x2F;03&#x2F;29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22&#x2F;03&#x2F;29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]#</p></blockquote></blockquote></blockquote><p>Spark On YARN模式</p><blockquote><blockquote><blockquote><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无</p></blockquote></blockquote></blockquote><p>需部署Spark集群, 只要找一台服务器, 充当Spark的客户端<br>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><blockquote><blockquote><blockquote><p>spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop 79 YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ….</p></blockquote></blockquote></blockquote><p>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）<br>bin&#x2F;pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下<br> bin&#x2F;spark-shell –master yarn –deploy-mode client|cluster<br>bin&#x2F;spark-submit –master yarn –deploy-mode client|cluster &#x2F;xxx&#x2F;xxx&#x2F;xxx.py 参数</p><blockquote><blockquote><blockquote></blockquote><p>  spark-submit 和 spark-shell 和 pyspark的相关参数</p></blockquote></blockquote><ul><li>bin&#x2F;pyspark: pyspark解释器spark环境 - bin&#x2F;spark-shell: scala解释器spark环境 - bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具 - bin&#x2F;spark-sql: sparksql客户端工具<br>  这4个客户端工具的参数基本通用.以spark-submit 为例: bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;master:7077 xxx.py&#96;<blockquote><blockquote><blockquote></blockquote><p>  Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>   Usage: spark-submit –kill [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit –status [submission ID] –master [spark:&#x2F;&#x2F;…]<br>   Usage: spark-submit run-example [options] example-class [example args] </p><blockquote></blockquote><p>   Options: –master MASTER_URL spark:&#x2F;&#x2F;host:port, mesos:&#x2F;&#x2F;host:port, yarn, k8s:&#x2F;&#x2F;<a href="https://host:port">https://host:port</a>, or </p><blockquote></blockquote><p>   local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java &#x2F; Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the </p><blockquote></blockquote><p>   driver and executor classpaths. –packages Comma-separated list of maven coordinates of </p><blockquote></blockquote><p>   jars to include on the driver and executor classpaths. Will </p><blockquote></blockquote><p>   search the local maven repo, then maven central and any </p><blockquote></blockquote><p>   additional remote repositories given by –repositories. The </p><blockquote></blockquote><p>   format for the coordinates should be </p><blockquote></blockquote><p>   groupId:artifactId:version.<br>   –exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br> – packages.<br> –py-files PY_FILES 指定Python程序依赖的其它python文件<br> –files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>  –archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br> –conf,<br> -c PROP&#x3D;VALUE 手动指定配置<br> –properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf&#x2F;spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.<br> –executor-memory MEM Executor的内存 (Default: 1G).<br> –proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br> –principal &#x2F;<br> –keytab.<br> –help,<br> -h 显示帮助文件<br>  –verbose,<br>  -v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):<br>   –driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>   –supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>   –kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:<br>   –total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:<br>    –executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):<br>    –num-executors NUM Executor应该开启几个<br>    –principal PRINCIPAL Principal to be used to login to KDC.<br>    –keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>    –queue QUEUE_NAME 指定运行的YARN队列(Default: “default”)</p></blockquote></blockquote></li></ul><p>启动 YARN 的历史服务器</p><blockquote><blockquote><blockquote><p>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin .&#x2F;mr-jobhistory-daemon.sh start historyserver</p></blockquote></blockquote></blockquote><p>访问WebUI界面</p><blockquote><blockquote><blockquote><p><a href="http://master:19888/">http://master:19888/</a></p></blockquote></blockquote></blockquote><p>client 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py</p></blockquote></blockquote></blockquote><p>cluster 模式测试</p><blockquote><blockquote><blockquote><p>SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark ${SPARK_HOME}&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” –conf “spark.pyspark.python&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;python3” ${SPARK_HOME}&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3**</p></blockquote></blockquote></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《Spark-HA-amp-Yarn配置》&quot;&gt;&lt;a href=&quot;#《Spark-HA-amp-Yarn配置》&quot; class=&quot;headerlink&quot; title=&quot;《Spark HA &amp;amp; Yarn配置》&quot;&gt;&lt;/a&gt;《Spark HA &amp;amp; Yarn配</summary>
      
    
    
    
    
  </entry>
  
</feed>
